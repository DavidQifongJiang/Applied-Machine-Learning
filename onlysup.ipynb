{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am9tTycrYh_p",
        "outputId": "9f11b962-23db-405b-e85c-c066793156e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting grid search for Logistic Regression:\n",
            "New best F1 for Logistic Regression: 0.7644 with params {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "New best F1 for Logistic Regression: 0.8670 with params {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "New best F1 for Logistic Regression: 0.8731 with params {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
            "New best F1 for Logistic Regression: 0.8788 with params {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "New best F1 for Logistic Regression: 0.8930 with params {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best F1 Score for logistic_regression: 0.8930 with parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Starting grid search for Naive Bayes:\n",
            "New best F1 for Naive Bayes: 0.8913 with params {'alpha': 0.1}\n",
            "New best F1 for Naive Bayes: 0.8937 with params {'alpha': 0.5}\n",
            "Best F1 Score for naive_bayes: 0.8937 with parameters: {'alpha': 0.5}\n",
            "\n",
            "Naive Bayes performed best.\n",
            "Best parameters: {'alpha': 0.5}\n",
            "Best F1 Score: 0.8937\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "trans = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Define the cleaning and preprocessing function\n",
        "def clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove @ mentions, URLs, numbers, and punctuation\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(trans)\n",
        "\n",
        "    # Remove stop words and extra whitespace\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words]).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Apply cleaning function\n",
        "train_data = clean_dataset(train_data)\n",
        "val_data = clean_dataset(val_data)\n",
        "\n",
        "# Separate labeled data (remove rows with -100 labels)\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "X_train = labeled_train['Phrase']\n",
        "y_train = labeled_train['Sentiment']\n",
        "\n",
        "# Validation data\n",
        "X_val = val_data['Phrase']\n",
        "y_val = val_data['Sentiment']\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(binary=True, min_df=3, stop_words=list(stop_words))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Parameter grids\n",
        "logreg_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']  # 'liblinear' supports L1, 'saga' supports both\n",
        "}\n",
        "\n",
        "nb_param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Grid search function\n",
        "def grid_search(X_train, y_train, X_val, y_val, model_type):\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    if model_type == 'logistic_regression':\n",
        "        param_grid = ParameterGrid(logreg_param_grid)\n",
        "        for params in param_grid:\n",
        "            try:\n",
        "                model = LogisticRegression(max_iter=1000, random_state=42, **params)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_val)\n",
        "                f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_params = params\n",
        "                    best_model = model\n",
        "                    print(f\"New best F1 for Logistic Regression: {f1:.4f} with params {best_params}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Skipped parameters {params} due to error: {e}\")\n",
        "\n",
        "    elif model_type == 'naive_bayes':\n",
        "        param_grid = ParameterGrid(nb_param_grid)\n",
        "        for params in param_grid:\n",
        "            model = MultinomialNB(**params)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_val)\n",
        "            f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_params = params\n",
        "                best_model = model\n",
        "                print(f\"New best F1 for Naive Bayes: {f1:.4f} with params {best_params}\")\n",
        "\n",
        "    print(f\"Best F1 Score for {model_type}: {best_f1:.4f} with parameters: {best_params}\")\n",
        "    return best_model, best_f1, best_params\n",
        "\n",
        "# Run grid search for both models\n",
        "print(\"Starting grid search for Logistic Regression:\")\n",
        "best_logreg_model, best_logreg_f1, best_logreg_params = grid_search(X_train_tfidf, y_train, X_val_tfidf, y_val, 'logistic_regression')\n",
        "\n",
        "print(\"\\nStarting grid search for Naive Bayes:\")\n",
        "best_nb_model, best_nb_f1, best_nb_params = grid_search(X_train_tfidf, y_train, X_val_tfidf, y_val, 'naive_bayes')\n",
        "\n",
        "# Compare results and select the best model\n",
        "if best_logreg_f1 > best_nb_f1:\n",
        "    print(\"\\nLogistic Regression performed best.\")\n",
        "    print(f\"Best parameters: {best_logreg_params}\")\n",
        "    print(f\"Best F1 Score: {best_logreg_f1:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNaive Bayes performed best.\")\n",
        "    print(f\"Best parameters: {best_nb_params}\")\n",
        "    print(f\"Best F1 Score: {best_nb_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "trans = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Define the cleaning and preprocessing function\n",
        "def clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove @ mentions, URLs, numbers, and punctuation\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(trans)\n",
        "\n",
        "    # Remove stop words and extra whitespace\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words]).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Apply cleaning function\n",
        "train_data = clean_dataset(train_data)\n",
        "val_data = clean_dataset(val_data)\n",
        "\n",
        "# Separate labeled data (remove rows with -100 labels)\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "X_train = labeled_train['Phrase']\n",
        "y_train = labeled_train['Sentiment']\n",
        "\n",
        "# Validation data\n",
        "X_val = val_data['Phrase']\n",
        "y_val = val_data['Sentiment']\n",
        "\n",
        "# Define parameter grids for each model\n",
        "vectorizer_param_grid = {\n",
        "    'min_df': [1, 3, 5],\n",
        "    'ngram_range': [(1, 1), (1, 2)]\n",
        "}\n",
        "\n",
        "logreg_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l2'],  # Only 'l2' to avoid compatibility issues\n",
        "    'solver': ['liblinear']  # 'liblinear' works well with l2 penalty\n",
        "}\n",
        "\n",
        "nb_param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Function to evaluate parameter combinations for a given model\n",
        "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    val_f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "    return val_f1\n",
        "\n",
        "# Grid search for Logistic Regression\n",
        "def grid_search_logistic_regression(X_train, y_train, X_val, y_val):\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "\n",
        "    for vec_params in ParameterGrid(vectorizer_param_grid):\n",
        "        vectorizer = TfidfVectorizer(binary=True, **vec_params, stop_words=list(stop_words))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler(with_mean=False)\n",
        "        X_train_scaled = scaler.fit_transform(X_train_tfidf)\n",
        "        X_val_scaled = scaler.transform(X_val_tfidf)\n",
        "\n",
        "        for logreg_params in ParameterGrid(logreg_param_grid):\n",
        "            model = LogisticRegression(max_iter=500, random_state=42, **logreg_params)\n",
        "            val_f1 = evaluate_model(model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                best_params = {'vectorizer': vec_params, 'logreg': logreg_params}\n",
        "                print(f\"New best F1 for Logistic Regression: {val_f1:.4f} with params {best_params}\")\n",
        "\n",
        "    print(f\"Best F1 Score for Logistic Regression: {best_f1:.4f} with parameters: {best_params}\")\n",
        "    return best_f1, best_params\n",
        "\n",
        "# Grid search for Naive Bayes\n",
        "def grid_search_naive_bayes(X_train, y_train, X_val, y_val):\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "\n",
        "    for vec_params in ParameterGrid(vectorizer_param_grid):\n",
        "        vectorizer = TfidfVectorizer(binary=True, **vec_params, stop_words=list(stop_words))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "        for nb_params in ParameterGrid(nb_param_grid):\n",
        "            model = MultinomialNB(**nb_params)\n",
        "            val_f1 = evaluate_model(model, X_train_tfidf, y_train, X_val_tfidf, y_val)\n",
        "\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                best_params = {'vectorizer': vec_params, 'nb': nb_params}\n",
        "                print(f\"New best F1 for Naive Bayes: {val_f1:.4f} with params {best_params}\")\n",
        "\n",
        "    print(f\"Best F1 Score for Naive Bayes: {best_f1:.4f} with parameters: {best_params}\")\n",
        "    return best_f1, best_params\n",
        "\n",
        "# Run grid search for both models\n",
        "print(\"Starting grid search for Logistic Regression:\")\n",
        "best_logreg_f1, best_logreg_params = grid_search_logistic_regression(X_train, y_train, X_val, y_val)\n",
        "\n",
        "print(\"\\nStarting grid search for Naive Bayes:\")\n",
        "best_nb_f1, best_nb_params = grid_search_naive_bayes(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Compare results and select the best model\n",
        "if best_logreg_f1 > best_nb_f1:\n",
        "    print(\"\\nLogistic Regression performed best.\")\n",
        "    print(f\"Best parameters: {best_logreg_params}\")\n",
        "    print(f\"Best F1 Score: {best_logreg_f1:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNaive Bayes performed best.\")\n",
        "    print(f\"Best parameters: {best_nb_params}\")\n",
        "    print(f\"Best F1 Score: {best_nb_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5kVIulye06w",
        "outputId": "61eb8371-92ec-4e21-c6be-66ad42f502a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting grid search for Logistic Regression:\n",
            "New best F1 for Logistic Regression: 0.8436 with params {'vectorizer': {'min_df': 1, 'ngram_range': (1, 1)}, 'logreg': {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}}\n",
            "New best F1 for Logistic Regression: 0.8891 with params {'vectorizer': {'min_df': 1, 'ngram_range': (1, 2)}, 'logreg': {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}}\n",
            "Best F1 Score for Logistic Regression: 0.8891 with parameters: {'vectorizer': {'min_df': 1, 'ngram_range': (1, 2)}, 'logreg': {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}}\n",
            "\n",
            "Starting grid search for Naive Bayes:\n",
            "New best F1 for Naive Bayes: 0.8932 with params {'vectorizer': {'min_df': 1, 'ngram_range': (1, 1)}, 'nb': {'alpha': 0.1}}\n",
            "New best F1 for Naive Bayes: 0.8965 with params {'vectorizer': {'min_df': 1, 'ngram_range': (1, 1)}, 'nb': {'alpha': 0.5}}\n",
            "New best F1 for Naive Bayes: 0.9229 with params {'vectorizer': {'min_df': 1, 'ngram_range': (1, 2)}, 'nb': {'alpha': 0.1}}\n",
            "Best F1 Score for Naive Bayes: 0.9229 with parameters: {'vectorizer': {'min_df': 1, 'ngram_range': (1, 2)}, 'nb': {'alpha': 0.1}}\n",
            "\n",
            "Naive Bayes performed best.\n",
            "Best parameters: {'vectorizer': {'min_df': 1, 'ngram_range': (1, 2)}, 'nb': {'alpha': 0.1}}\n",
            "Best F1 Score: 0.9229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "trans = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Define the cleaning and preprocessing function\n",
        "def clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove @ mentions, URLs, numbers, and punctuation\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(trans)\n",
        "\n",
        "    # Remove stop words and extra whitespace\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words]).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Apply cleaning function\n",
        "train_data = clean_dataset(train_data)\n",
        "val_data = clean_dataset(val_data)\n",
        "\n",
        "# Separate labeled data (remove rows with -100 labels)\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "X_train = labeled_train['Phrase']\n",
        "y_train = labeled_train['Sentiment']\n",
        "\n",
        "# Validation data\n",
        "X_val = val_data['Phrase']\n",
        "y_val = val_data['Sentiment']\n",
        "\n",
        "# Use the best parameters for TF-IDF vectorizer and Naive Bayes\n",
        "vectorizer = TfidfVectorizer(binary=True, min_df=1, ngram_range=(1, 2), stop_words=list(stop_words))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Train Naive Bayes model with the best alpha\n",
        "model = MultinomialNB(alpha=0.1)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "val_accuracy = accuracy_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"F1 Score on Validation Set: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z7PAJGrpxHg",
        "outputId": "a84ea50d-d307-4444-c3c7-14fc21eb9d20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9229\n",
            "F1 Score on Validation Set: 0.9229\n"
          ]
        }
      ]
    }
  ]
}