{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E6MdBzgGQcf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc333ce-4761-4bbf-bf42-1b45b314edff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularization: L1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6969\n",
            "F1 Score on Validation Set: 0.6983\n",
            "\n",
            "Regularization: L2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7041\n",
            "F1 Score on Validation Set: 0.7052\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import vstack\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define text cleaning function\n",
        "def clean(text):\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', str(text))  # Remove URLs\n",
        "    text = re.sub(r\"<br />\", \" \", text)\n",
        "    text = re.sub(r\"&quot;\", \"\\\"\", text)\n",
        "    text = re.sub('&#39;', \"\\\"\", text)\n",
        "    text = re.sub('\\n', \" \", text)\n",
        "    text = re.sub(' u ', \" you \", text)\n",
        "    text = re.sub('`', \"\", text)\n",
        "    text = re.sub(r\"(!)\\1+\", r\"!\", text)\n",
        "    text = re.sub(r\"(\\?)\\1+\", r\"?\", text)\n",
        "    text = re.sub('&amp;', 'and', text)\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Substitute multiple spaces with a single space\n",
        "    text = text.lower().strip()  # Convert to lowercase and strip whitespace\n",
        "    return text\n",
        "\n",
        "# Load and clean data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "train_data['Phrase'] = train_data['Phrase'].astype(str).apply(clean)\n",
        "val_data['Phrase'] = val_data['Phrase'].astype(str).apply(clean)\n",
        "\n",
        "# Define stop words for Bag of Words\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# Vectorization function with Bag of Words (BoW)\n",
        "def phrase_vectorize(train_data):\n",
        "    vectorizer = CountVectorizer(binary=True, min_df=5, stop_words=stop_words)  # Bag of Words (BoW)\n",
        "    vectorizer.fit(train_data['Phrase'])\n",
        "    return vectorizer\n",
        "\n",
        "# Unsupervised learning with K-Means\n",
        "def unsup_learn(K, X_train_unlabeled, X_train_labeled, y_train_labeled):\n",
        "    pca = PCA(n_components=200, random_state=42)\n",
        "    X_train_unlabeled_reduced = pca.fit_transform(X_train_unlabeled)\n",
        "    X_train_labeled_reduced = pca.transform(X_train_labeled)\n",
        "\n",
        "    learn_model = KMeans(n_clusters=K, random_state=42)\n",
        "    pseudo_labels = learn_model.fit_predict(X_train_unlabeled_reduced)\n",
        "\n",
        "    # Match clusters to labels\n",
        "    cost_matrix = np.zeros((K, K))\n",
        "    for cluster_id in range(K):\n",
        "        labeled_in_cluster = y_train_labeled[learn_model.predict(X_train_labeled_reduced) == cluster_id]\n",
        "        for label in range(K):\n",
        "            cost_matrix[cluster_id, label] = np.sum(labeled_in_cluster != label)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    cluster_to_label_map = {row: col for row, col in zip(row_ind, col_ind)}\n",
        "    pseudo_labels_remapped = np.array([cluster_to_label_map[cluster] for cluster in pseudo_labels])\n",
        "\n",
        "    X_combined = vstack([X_train_labeled, X_train_unlabeled])\n",
        "    y_combined = np.concatenate((y_train_labeled, pseudo_labels_remapped))\n",
        "    return X_combined, y_combined\n",
        "\n",
        "# Supervised learning with LogisticRegression for L1 and L2 regularization\n",
        "def sup_learn(X_combined, y_combined, X_val, y_val, regularization):\n",
        "    if regularization == \"L1\":\n",
        "        learn_model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000, penalty='l1')  # L1 regularization\n",
        "    elif regularization == \"L2\":\n",
        "        learn_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, penalty='l2')  # L2 regularization\n",
        "    else:\n",
        "        raise ValueError(\"Invalid regularization method. Use 'L1' or 'L2'.\")\n",
        "\n",
        "    learn_model.fit(X_combined, y_combined)\n",
        "    y_pred_val = learn_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "    f1 = f1_score(y_val, y_pred_val, average=\"macro\")\n",
        "    return val_accuracy, f1\n",
        "\n",
        "# Main training function for testing L1 and L2 regularization only\n",
        "def final_training():\n",
        "    my_vectorizer = phrase_vectorize(train_data)\n",
        "    X_train_labeled = my_vectorizer.transform(labeled_train['Phrase']).toarray()\n",
        "    X_val_labeled = my_vectorizer.transform(labeled_val['Phrase']).toarray()\n",
        "    X_unlabeled = my_vectorizer.transform(unlabeled_train['Phrase']).toarray()\n",
        "\n",
        "    y_train_labeled = labeled_train['Sentiment']\n",
        "    y_val = labeled_val['Sentiment']\n",
        "    K = len(np.unique(y_train_labeled))\n",
        "    X_combined, y_combined = unsup_learn(K, X_unlabeled, X_train_labeled, y_train_labeled)\n",
        "\n",
        "    # Only iterate over L1 and L2 regularization methods\n",
        "    for reg in [\"L1\", \"L2\"]:\n",
        "        print(f\"Regularization: {reg}\")\n",
        "        val_accuracy, f1 = sup_learn(X_combined, y_combined, X_val_labeled, y_val, reg)\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        print(f\"F1 Score on Validation Set: {f1:.4f}\\n\")\n",
        "\n",
        "# Separate labeled and unlabeled data\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "unlabeled_train = train_data[train_data['Sentiment'] == -100]\n",
        "labeled_val = val_data[val_data['Sentiment'] != -100]\n",
        "\n",
        "# Run final training\n",
        "final_training()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rZoiQsnTo35T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Apply the clean function to the first column of a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # Assuming dataset is a pandas DataFrame and 'Phrase' is the text column\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "def tokenize_lexicon(texts):\n",
        "    \"\"\"\n",
        "    Tokenize and POS tag each text in the list.\n",
        "    \"\"\"\n",
        "    return [nltk.pos_tag(nltk.word_tokenize(text)) for text in texts]\n",
        "\n",
        "def get_wordnet_pos(pos_tag):\n",
        "    \"\"\"\n",
        "    Map NLTK POS tags to WordNet POS tags.\n",
        "    \"\"\"\n",
        "    return TAG_DICT.get(pos_tag[0], wn.NOUN)\n",
        "\n",
        "def lemmatize_texts(texts):\n",
        "    \"\"\"\n",
        "    Lemmatize each word in the tokenized and POS-tagged texts.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        [lemmer.lemmatize(word, pos=get_wordnet_pos(pos_tag)) for word, pos_tag in text]\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "def stem_texts(texts):\n",
        "    \"\"\"\n",
        "    Stem each word in the tokenized texts.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        [stemmer.stem(word) for word, _ in text]\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "def backtostring(texts):\n",
        "    \"\"\"\n",
        "    Convert lists of tokens back into strings.\n",
        "    \"\"\"\n",
        "    return [\" \".join(text) for text in texts]"
      ],
      "metadata": {
        "id": "uO2xBJ0Co4tn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from scipy.sparse import vstack\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "lemmer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tag dictionary for POS mapping\n",
        "TAG_DICT = {\n",
        "    'J': wn.ADJ,\n",
        "    'V': wn.VERB,\n",
        "    'N': wn.NOUN,\n",
        "    'R': wn.ADV\n",
        "}\n",
        "\n",
        "# Define cleaning and preprocessing functions\n",
        "def clean(text):\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', str(text))  # Remove URLs\n",
        "    text = re.sub(r\"<br />\", \" \", text)\n",
        "    text = re.sub(r\"&quot;\", \"\\\"\", text)\n",
        "    text = re.sub('&#39;', \"\\\"\", text)\n",
        "    text = re.sub('\\n', \" \", text)\n",
        "    text = re.sub(' u ', \" you \", text)\n",
        "    text = re.sub('`', \"\", text)\n",
        "    text = re.sub(r\"(!)\\1+\", r\"!\", text)\n",
        "    text = re.sub(r\"(\\?)\\1+\", r\"?\", text)\n",
        "    text = re.sub('&amp;', 'and', text)\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Substitute multiple spaces with a single space\n",
        "    text = text.lower().strip()  # Convert to lowercase and strip whitespace\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "def tokenize_lexicon(texts):\n",
        "    return [nltk.pos_tag(nltk.word_tokenize(text)) for text in texts]\n",
        "\n",
        "def get_wordnet_pos(pos_tag):\n",
        "    return TAG_DICT.get(pos_tag[0], wn.NOUN)\n",
        "\n",
        "def lemmatize_texts(texts):\n",
        "    return [\n",
        "        [lemmer.lemmatize(word, pos=get_wordnet_pos(pos_tag)) for word, pos_tag in text]\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "def stem_texts(texts):\n",
        "    \"\"\"\n",
        "    Stem each word in the tokenized texts.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        [stemmer.stem(word) for word in text]  # Stem each word without needing POS tags\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "def backtostring(texts):\n",
        "    return [\" \".join(text) for text in texts]\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Apply cleaning, tokenization, lemmatization, stemming, and conversion back to strings\n",
        "train_data = clean_dataset(train_data)\n",
        "val_data = clean_dataset(val_data)\n",
        "\"\"\"\n",
        "# Tokenization and POS tagging\n",
        "train_data['tokens'] = tokenize_lexicon(train_data['Phrase'])\n",
        "val_data['tokens'] = tokenize_lexicon(val_data['Phrase'])\n",
        "\n",
        "# Lemmatization\n",
        "train_data['tokens'] = lemmatize_texts(train_data['tokens'])\n",
        "val_data['tokens'] = lemmatize_texts(val_data['tokens'])\n",
        "\n",
        "\n",
        "# Convert tokens back to strings\n",
        "train_data['Phrase'] = backtostring(train_data['tokens'])\n",
        "val_data['Phrase'] = backtostring(val_data['tokens'])\n",
        "\"\"\"\n",
        "# Define stop words for TF-IDF\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# Vectorization function with TF-IDF unigram\n",
        "def phrase_vectorize(train_data):\n",
        "    vectorizer = TfidfVectorizer(binary=True, min_df=3, stop_words=stop_words)  # TF-IDF with unigrams\n",
        "    vectorizer.fit(train_data['Phrase'])\n",
        "    return vectorizer\n",
        "\n",
        "# Unsupervised learning with Gaussian Mixture Model (GMM)\n",
        "def unsup_learn(K, X_train_unlabeled, X_train_labeled, y_train_labeled):\n",
        "    pca = PCA(n_components=200, random_state=42)\n",
        "    X_train_unlabeled_reduced = pca.fit_transform(X_train_unlabeled)\n",
        "    X_train_labeled_reduced = pca.transform(X_train_labeled)\n",
        "\n",
        "    learn_model = GaussianMixture(n_components=K, covariance_type='diag', random_state=42, max_iter=50, tol=1e-3)\n",
        "    pseudo_labels = learn_model.fit_predict(X_train_unlabeled_reduced)\n",
        "\n",
        "    # Match clusters to labels\n",
        "    cost_matrix = np.zeros((K, K))\n",
        "    for cluster_id in range(K):\n",
        "        labeled_in_cluster = y_train_labeled[learn_model.predict(X_train_labeled_reduced) == cluster_id]\n",
        "        for label in range(K):\n",
        "            cost_matrix[cluster_id, label] = np.sum(labeled_in_cluster != label)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    cluster_to_label_map = {row: col for row, col in zip(row_ind, col_ind)}\n",
        "    pseudo_labels_remapped = np.array([cluster_to_label_map[cluster] for cluster in pseudo_labels])\n",
        "\n",
        "    X_combined = vstack([X_train_labeled, X_train_unlabeled])\n",
        "    y_combined = np.concatenate((y_train_labeled, pseudo_labels_remapped))\n",
        "    return X_combined, y_combined\n",
        "\n",
        "# Supervised learning with MultinomialNB\n",
        "def sup_learn(X_combined, y_combined, X_val, y_val):\n",
        "    learn_model = MultinomialNB(alpha=1)  # Multinomial Naive Bayes with smoothing\n",
        "    learn_model.fit(X_combined, y_combined)\n",
        "    y_pred_val = learn_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "    f1 = f1_score(y_val, y_pred_val, average=\"macro\")\n",
        "    return val_accuracy, f1\n",
        "\n",
        "# Main training function\n",
        "def final_training():\n",
        "    my_vectorizer = phrase_vectorize(train_data)\n",
        "    X_train_labeled = my_vectorizer.transform(labeled_train['Phrase']).toarray()\n",
        "    X_val_labeled = my_vectorizer.transform(labeled_val['Phrase']).toarray()\n",
        "    X_unlabeled = my_vectorizer.transform(unlabeled_train['Phrase']).toarray()\n",
        "\n",
        "    y_train_labeled = labeled_train['Sentiment']\n",
        "    y_val = labeled_val['Sentiment']\n",
        "    K = len(np.unique(y_train_labeled))\n",
        "    X_combined, y_combined = unsup_learn(K, X_unlabeled, X_train_labeled, y_train_labeled)\n",
        "\n",
        "    # Evaluate with MultinomialNB\n",
        "    print(\"Using MultinomialNB:\")\n",
        "    val_accuracy, f1 = sup_learn(X_combined, y_combined, X_val_labeled, y_val)\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"F1 Score on Validation Set: {f1:.4f}\")\n",
        "\n",
        "# Separate labeled and unlabeled data\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "unlabeled_train = train_data[train_data['Sentiment'] == -100]\n",
        "labeled_val = val_data[val_data['Sentiment'] != -100]\n",
        "\n",
        "# Run final training\n",
        "final_training()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsnvslKenr03",
        "outputId": "cc2ca0d7-0cbb-46ff-b078-4f919c9bc9f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using MultinomialNB:\n",
            "Validation Accuracy: 0.7891\n",
            "F1 Score on Validation Set: 0.7877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from scipy.sparse import vstack\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "trans = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Define the cleaning and preprocessing function\n",
        "def clean(text):\n",
        "    # Ensure text is a string, if not return an empty string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove @ mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(trans)\n",
        "\n",
        "    # Remove stop words\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Strip extra whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Apply cleaning function to train and validation datasets\n",
        "train_data = clean_dataset(train_data)\n",
        "val_data = clean_dataset(val_data)\n",
        "\n",
        "# Vectorization function with TF-IDF unigram\n",
        "# Vectorization function with TF-IDF unigram\n",
        "def phrase_vectorize(train_data):\n",
        "    vectorizer = TfidfVectorizer(binary=True, min_df=5, stop_words=list(stop_words))  # Convert stop_words to a list\n",
        "    vectorizer.fit(train_data['Phrase'])\n",
        "    return vectorizer\n",
        "\n",
        "\n",
        "# Unsupervised learning with Gaussian Mixture Model (GMM)\n",
        "def unsup_learn(K, X_train_unlabeled, X_train_labeled, y_train_labeled):\n",
        "    pca = PCA(n_components=50, random_state=42)\n",
        "    X_train_unlabeled_reduced = pca.fit_transform(X_train_unlabeled)\n",
        "    X_train_labeled_reduced = pca.transform(X_train_labeled)\n",
        "\n",
        "    learn_model = GaussianMixture(n_components=K, covariance_type='full', random_state=42, max_iter=50, tol=1e-3)\n",
        "    pseudo_labels = learn_model.fit_predict(X_train_unlabeled_reduced)\n",
        "\n",
        "    # Match clusters to labels\n",
        "    cost_matrix = np.zeros((K, K))\n",
        "    for cluster_id in range(K):\n",
        "        labeled_in_cluster = y_train_labeled[learn_model.predict(X_train_labeled_reduced) == cluster_id]\n",
        "        for label in range(K):\n",
        "            cost_matrix[cluster_id, label] = np.sum(labeled_in_cluster != label)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    cluster_to_label_map = {row: col for row, col in zip(row_ind, col_ind)}\n",
        "    pseudo_labels_remapped = np.array([cluster_to_label_map[cluster] for cluster in pseudo_labels])\n",
        "\n",
        "    X_combined = vstack([X_train_labeled, X_train_unlabeled])\n",
        "    y_combined = np.concatenate((y_train_labeled, pseudo_labels_remapped))\n",
        "    return X_combined, y_combined\n",
        "\n",
        "# Supervised learning with MultinomialNB\n",
        "def sup_learn(X_combined, y_combined, X_val, y_val):\n",
        "    learn_model = MultinomialNB(alpha=0.5)  # Multinomial Naive Bayes with smoothing\n",
        "    learn_model.fit(X_combined, y_combined)\n",
        "    y_pred_val = learn_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "    f1 = f1_score(y_val, y_pred_val, average=\"macro\")\n",
        "    return val_accuracy, f1\n",
        "\n",
        "# Main training function\n",
        "def final_training():\n",
        "    my_vectorizer = phrase_vectorize(train_data)\n",
        "    X_train_labeled = my_vectorizer.transform(labeled_train['Phrase']).toarray()\n",
        "    X_val_labeled = my_vectorizer.transform(labeled_val['Phrase']).toarray()\n",
        "    X_unlabeled = my_vectorizer.transform(unlabeled_train['Phrase']).toarray()\n",
        "\n",
        "    y_train_labeled = labeled_train['Sentiment']\n",
        "    y_val = labeled_val['Sentiment']\n",
        "    K = len(np.unique(y_train_labeled))\n",
        "    X_combined, y_combined = unsup_learn(K, X_unlabeled, X_train_labeled, y_train_labeled)\n",
        "\n",
        "    # Evaluate with MultinomialNB\n",
        "    print(\"Using MultinomialNB:\")\n",
        "    val_accuracy, f1 = sup_learn(X_combined, y_combined, X_val_labeled, y_val)\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"F1 Score on Validation Set: {f1:.4f}\")\n",
        "\n",
        "# Separate labeled and unlabeled data\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "unlabeled_train = train_data[train_data['Sentiment'] == -100]\n",
        "labeled_val = val_data[val_data['Sentiment'] != -100]\n",
        "\n",
        "# Run final training\n",
        "final_training()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o1tH6c37iur",
        "outputId": "96f944aa-9930-4c3e-8469-208226eb4265"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using MultinomialNB:\n",
            "Validation Accuracy: 0.8467\n",
            "F1 Score on Validation Set: 0.8484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from scipy.sparse import vstack\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "trans = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Define the cleaning and preprocessing function\n",
        "def clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove @ mentions, URLs, numbers, and punctuation\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(trans)\n",
        "\n",
        "    # Remove stop words and extra whitespace\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words]).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    dataset['Phrase'] = dataset['Phrase'].apply(clean)\n",
        "    return dataset\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "train_data = clean_dataset(train_data)\n",
        "val_data = clean_dataset(val_data)\n",
        "\n",
        "# Vectorization function with TF-IDF unigram\n",
        "def phrase_vectorize(train_data, min_df):\n",
        "    vectorizer = TfidfVectorizer(binary=True, min_df=min_df, stop_words=list(stop_words))\n",
        "    vectorizer.fit(train_data['Phrase'])\n",
        "    return vectorizer\n",
        "\n",
        "# Unsupervised learning with Gaussian Mixture Model (GMM)\n",
        "def unsup_learn(K, X_train_unlabeled, X_train_labeled, y_train_labeled, n_components=100, covariance_type='diag'):\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    X_train_unlabeled_reduced = pca.fit_transform(X_train_unlabeled)\n",
        "    X_train_labeled_reduced = pca.transform(X_train_labeled)\n",
        "\n",
        "    learn_model = GaussianMixture(n_components=K, covariance_type=covariance_type, random_state=42)\n",
        "    pseudo_labels = learn_model.fit_predict(X_train_unlabeled_reduced)\n",
        "\n",
        "    # Match clusters to labels\n",
        "    cost_matrix = np.zeros((K, K))\n",
        "    for cluster_id in range(K):\n",
        "        labeled_in_cluster = y_train_labeled[learn_model.predict(X_train_labeled_reduced) == cluster_id]\n",
        "        for label in range(K):\n",
        "            cost_matrix[cluster_id, label] = np.sum(labeled_in_cluster != label)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    cluster_to_label_map = {row: col for row, col in zip(row_ind, col_ind)}\n",
        "    pseudo_labels_remapped = np.array([cluster_to_label_map[cluster] for cluster in pseudo_labels])\n",
        "\n",
        "    X_combined = vstack([X_train_labeled, X_train_unlabeled])\n",
        "    y_combined = np.concatenate((y_train_labeled, pseudo_labels_remapped))\n",
        "    return X_combined, y_combined\n",
        "\n",
        "# Supervised learning with MultinomialNB\n",
        "def sup_learn(X_combined, y_combined, X_val, y_val, alpha):\n",
        "    learn_model = MultinomialNB(alpha=alpha)\n",
        "    learn_model.fit(X_combined, y_combined)\n",
        "    y_pred_val = learn_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "    f1 = f1_score(y_val, y_pred_val, average=\"macro\")\n",
        "    return val_accuracy, f1\n",
        "\n",
        "# Grid search for best parameters\n",
        "def grid_search(train_data, val_data):\n",
        "    # Set up the parameter grid\n",
        "    param_grid = {\n",
        "        'alpha': [0.5],\n",
        "        'n_components': [50, 100, 150,200],\n",
        "        'covariance_type': ['full'],\n",
        "        'min_df': [1, 2,3,4, 5,6,7]\n",
        "    }\n",
        "    grid = ParameterGrid(param_grid)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "\n",
        "    X_val_labeled = None  # Initialize variable outside the loop for reuse\n",
        "\n",
        "    # Loop over each parameter combination\n",
        "    for params in grid:\n",
        "        print(f\"Testing parameters: {params}\")\n",
        "\n",
        "        # Vectorize with the current min_df\n",
        "        my_vectorizer = phrase_vectorize(train_data, min_df=params['min_df'])\n",
        "\n",
        "        # Transform train, val, and unlabeled data\n",
        "        X_train_labeled = my_vectorizer.transform(labeled_train['Phrase']).toarray()\n",
        "        X_val_labeled = my_vectorizer.transform(labeled_val['Phrase']).toarray()\n",
        "        X_unlabeled = my_vectorizer.transform(unlabeled_train['Phrase']).toarray()\n",
        "\n",
        "        y_train_labeled = labeled_train['Sentiment']\n",
        "        y_val = labeled_val['Sentiment']\n",
        "        K = len(np.unique(y_train_labeled))\n",
        "\n",
        "        # Unsupervised learning with current params\n",
        "        X_combined, y_combined = unsup_learn(K, X_unlabeled, X_train_labeled, y_train_labeled,\n",
        "                                             n_components=params['n_components'],\n",
        "                                             covariance_type=params['covariance_type'])\n",
        "\n",
        "        # Supervised learning with current params\n",
        "        val_accuracy, f1 = sup_learn(X_combined, y_combined, X_val_labeled, y_val, alpha=params['alpha'])\n",
        "\n",
        "        # Check if this is the best so far\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_params = params\n",
        "            print(f\"New best F1 score: {f1:.4f} with params {best_params}\")\n",
        "\n",
        "    print(f\"Best F1 Score: {best_f1:.4f} with parameters: {best_params}\")\n",
        "\n",
        "# Separate labeled and unlabeled data\n",
        "labeled_train = train_data[train_data['Sentiment'] != -100]\n",
        "unlabeled_train = train_data[train_data['Sentiment'] == -100]\n",
        "labeled_val = val_data[val_data['Sentiment'] != -100]\n",
        "\n",
        "# Run grid search to find the best parameters\n",
        "grid_search(train_data, val_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBLdEcaF-kne",
        "outputId": "d736f3fb-108a-4196-8705-fc0f47762113"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 1, 'n_components': 50}\n",
            "New best F1 score: 0.7941 with params {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 1, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 1, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 1, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 1, 'n_components': 200}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 2, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 2, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 2, 'n_components': 150}\n",
            "New best F1 score: 0.8684 with params {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 2, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 2, 'n_components': 200}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 3, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 3, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 3, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 3, 'n_components': 200}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 4, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 4, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 4, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 4, 'n_components': 200}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 5, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 5, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 5, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 5, 'n_components': 200}\n",
            "New best F1 score: 0.8728 with params {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 5, 'n_components': 200}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 6, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 6, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 6, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 6, 'n_components': 200}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 7, 'n_components': 50}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 7, 'n_components': 100}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 7, 'n_components': 150}\n",
            "Testing parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 7, 'n_components': 200}\n",
            "Best F1 Score: 0.8728 with parameters: {'alpha': 0.5, 'covariance_type': 'full', 'min_df': 5, 'n_components': 200}\n"
          ]
        }
      ]
    }
  ]
}